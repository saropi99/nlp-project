{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b84bcd",
   "metadata": {},
   "source": [
    " Employ [Hugging Face](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=sentiment) transformers for the same classification task as in the first assignment.\n",
    "\n",
    "Explore Hugging Face models to find a pre-trained model that is suitable and promising for fine-tuning to your task. It should make sense to pick one that has been pre-trained for the same language and/or text genre.\n",
    "\n",
    "As a bonus, you can also employ a [domain adaptation](https://huggingface.co/learn/llm-course/chapter7/3?fw=pt) approach, explore [parameter-efficient fine-tuning](https://huggingface.co/docs/peft/main/quicktour) (e.g. LoRA), or [prompting language models](https://huggingface.co/docs/transformers/v4.49.0/en/tasks/prompting).\n",
    "\n",
    "We must ompare the performance of your model(s) with the ones developed for the first assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560a55d",
   "metadata": {},
   "source": [
    "Most of the models have problems processing the text!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2ec1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import CustomDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "493a620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sentiment_df = pd.read_csv(\"../common/data_sentiment_preprocessed.csv\")\n",
    "combined_sentiment_df_val = pd.read_csv(\"../common/data_sentiment_preprocessed_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99de155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = combined_sentiment_df.text\n",
    "y_train = combined_sentiment_df.sentiment_label\n",
    "x_val = combined_sentiment_df_val.text\n",
    "y_val = combined_sentiment_df_val.sentiment_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c4d1f",
   "metadata": {},
   "source": [
    "## Making use of pretrained huggingface models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e32128",
   "metadata": {},
   "source": [
    "### twitter-roberta-base-sentiment-latest\n",
    "This model has positive negative and neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98fef923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9748510718345642}]\n",
      "[{'label': 'negative', 'score': 0.8965417742729187}]\n",
      "[{'label': 'neutral', 'score': 0.7886281609535217}]\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "from transformers import pipeline\n",
    "\n",
    "cardiffnlp_roberta = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "\n",
    "print(cardiffnlp_roberta(\"I love you!\"))\n",
    "print(cardiffnlp_roberta(\"I hate you!\"))\n",
    "print(cardiffnlp_roberta(\"neutral text\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e00cb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text at index 697: Positives: First time going to this place today. Let me tell you, coming from a family of chefs this was delectable, the dine in meals came out fast, they were LARGE portions, and very good temperature. We ordered the flowered onion( fried and whole), we ordered the Louisiana chook both entrees. Then I had the parmigiana as my main with mash and veg. The mash and veg was perfectly cooked, though the mash tastes a little like packet mash. The sauce with the Louisiana chicken is a little spicy so if you ca n’t tolerate a little spice the sauce is n’t for you. But man oh man the crunch on the chook and the juicy chicken was incredible, was thoroughly enjoyable. The parmigiana was LARGE so much so I could n’t finish it all. Great that they gave takeaways Negatives: The drink I ordered was the summer one in the mocktails section, tasted great only issue I really had was the lemon seeds in the drink, lucky the straws were n’t big enough to suck them up otherwise we would had an issue trying to fish multiple seeds out of the drink. We loved the Louisiana chicken so much we ordered some to take away, sadly to our disappointment the portion sizes were very different, much smaller and not as crunchy still very tasty and very juicy just not the same portion sizes as dine in( specifically regarding our dining experience) We also ordered take away chocolate churro and fruit fondu. The Churros are Exquisite as far as crunch goes but there is little to no cinnamon sugar on it. But the lack of cinnamon sugar can be covered up by the chocolate sauce and fruit eaten together bite by bite. The sauce also tasted Store bought which is still tasted good to me but may not be to everyone liking. The flowered onion, leaves much to be desired for, I definitely would n’t eat it by itself as it essentially just a whole onion fried up. The sauce was nice and it goes well with the main meal. Other: I must say they brought the mains out not long after we got our entrees which is okay. Just did n’t really get time to savour the entrees. The serves seems a little all over the place but generally very welcoming and accomodating. Also Incase anyone wants to know they have the MCCS discounts( for marines) Over all my dining experience was fantastic. Just a little disappointed with the takeaway serving sizes. And the wait for takeaway was MUCH longer than the wait for the main meal. Taste for those specific meals( besides the onion and the lack of cinnamon sugar) was great. Will definitely bring my husband and other family here again:)\n",
      "The expanded size of the tensor (574) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 574].  Tensor sizes: [1, 514]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.66      0.78       611\n",
      "           1       0.73      0.97      0.83       600\n",
      "\n",
      "    accuracy                           0.81      1211\n",
      "   macro avg       0.84      0.81      0.81      1211\n",
      "weighted avg       0.84      0.81      0.81      1211\n",
      "\n",
      "0.8100743187448389\n"
     ]
    }
   ],
   "source": [
    "# cardiffnlp_roberta\n",
    "mapper = {\n",
    "    \"negative\": 0,\n",
    "    \"positive\": 1,\n",
    "    \"neutral\": 1\n",
    "}\n",
    "utils.apply_kaggle_model(cardiffnlp_roberta, mapper, x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1c04a",
   "metadata": {},
   "source": [
    "### sentiment-roberta-large-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "badf0cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9987329840660095}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9992897510528564}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9969080090522766}]\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/siebert/sentiment-roberta-large-english?library=transformers\n",
    "\n",
    "\"\"\"\n",
    "    article: https://www.sciencedirect.com/science/article/pii/S0167811622000477\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "siebert_roberta = pipeline(\"text-classification\", model=\"siebert/sentiment-roberta-large-english\")\n",
    "\n",
    "\n",
    "print(siebert_roberta(\"I love you!\"))\n",
    "print(siebert_roberta(\"I hate you!\"))\n",
    "print(siebert_roberta(\"neutral text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fba1a134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text at index 697: Positives: First time going to this place today. Let me tell you, coming from a family of chefs this was delectable, the dine in meals came out fast, they were LARGE portions, and very good temperature. We ordered the flowered onion( fried and whole), we ordered the Louisiana chook both entrees. Then I had the parmigiana as my main with mash and veg. The mash and veg was perfectly cooked, though the mash tastes a little like packet mash. The sauce with the Louisiana chicken is a little spicy so if you ca n’t tolerate a little spice the sauce is n’t for you. But man oh man the crunch on the chook and the juicy chicken was incredible, was thoroughly enjoyable. The parmigiana was LARGE so much so I could n’t finish it all. Great that they gave takeaways Negatives: The drink I ordered was the summer one in the mocktails section, tasted great only issue I really had was the lemon seeds in the drink, lucky the straws were n’t big enough to suck them up otherwise we would had an issue trying to fish multiple seeds out of the drink. We loved the Louisiana chicken so much we ordered some to take away, sadly to our disappointment the portion sizes were very different, much smaller and not as crunchy still very tasty and very juicy just not the same portion sizes as dine in( specifically regarding our dining experience) We also ordered take away chocolate churro and fruit fondu. The Churros are Exquisite as far as crunch goes but there is little to no cinnamon sugar on it. But the lack of cinnamon sugar can be covered up by the chocolate sauce and fruit eaten together bite by bite. The sauce also tasted Store bought which is still tasted good to me but may not be to everyone liking. The flowered onion, leaves much to be desired for, I definitely would n’t eat it by itself as it essentially just a whole onion fried up. The sauce was nice and it goes well with the main meal. Other: I must say they brought the mains out not long after we got our entrees which is okay. Just did n’t really get time to savour the entrees. The serves seems a little all over the place but generally very welcoming and accomodating. Also Incase anyone wants to know they have the MCCS discounts( for marines) Over all my dining experience was fantastic. Just a little disappointed with the takeaway serving sizes. And the wait for takeaway was MUCH longer than the wait for the main meal. Taste for those specific meals( besides the onion and the lack of cinnamon sugar) was great. Will definitely bring my husband and other family here again:)\n",
      "The expanded size of the tensor (574) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 574].  Tensor sizes: [1, 514]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.84      0.88       611\n",
      "           1       0.85      0.93      0.89       600\n",
      "\n",
      "    accuracy                           0.88      1211\n",
      "   macro avg       0.88      0.88      0.88      1211\n",
      "weighted avg       0.88      0.88      0.88      1211\n",
      "\n",
      "0.8810900082576383\n"
     ]
    }
   ],
   "source": [
    "#siebert_roberta\n",
    "mapper = {\n",
    "    \"NEGATIVE\": 0,\n",
    "    \"POSITIVE\": 1\n",
    "} \n",
    "utils.apply_kaggle_model(siebert_roberta, mapper, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7099805",
   "metadata": {},
   "source": [
    "### AG6019/reddit-comment-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc8ebe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9998805522918701}]\n",
      "[{'label': 'LABEL_0', 'score': 0.8577982187271118}]\n",
      "[{'label': 'LABEL_0', 'score': 0.9975653886795044}]\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/AG6019/reddit-comment-sentiment?library=transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "AG6019_comment = pipeline(\"text-classification\", model=\"AG6019/reddit-comment-sentiment\")\n",
    "\n",
    "print(AG6019_comment(\"I love you!\"))\n",
    "print(AG6019_comment(\"I dont like you!\"))\n",
    "print(AG6019_comment(\"neutral text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de49a7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text at index 697: Positives: First time going to this place today. Let me tell you, coming from a family of chefs this was delectable, the dine in meals came out fast, they were LARGE portions, and very good temperature. We ordered the flowered onion( fried and whole), we ordered the Louisiana chook both entrees. Then I had the parmigiana as my main with mash and veg. The mash and veg was perfectly cooked, though the mash tastes a little like packet mash. The sauce with the Louisiana chicken is a little spicy so if you ca n’t tolerate a little spice the sauce is n’t for you. But man oh man the crunch on the chook and the juicy chicken was incredible, was thoroughly enjoyable. The parmigiana was LARGE so much so I could n’t finish it all. Great that they gave takeaways Negatives: The drink I ordered was the summer one in the mocktails section, tasted great only issue I really had was the lemon seeds in the drink, lucky the straws were n’t big enough to suck them up otherwise we would had an issue trying to fish multiple seeds out of the drink. We loved the Louisiana chicken so much we ordered some to take away, sadly to our disappointment the portion sizes were very different, much smaller and not as crunchy still very tasty and very juicy just not the same portion sizes as dine in( specifically regarding our dining experience) We also ordered take away chocolate churro and fruit fondu. The Churros are Exquisite as far as crunch goes but there is little to no cinnamon sugar on it. But the lack of cinnamon sugar can be covered up by the chocolate sauce and fruit eaten together bite by bite. The sauce also tasted Store bought which is still tasted good to me but may not be to everyone liking. The flowered onion, leaves much to be desired for, I definitely would n’t eat it by itself as it essentially just a whole onion fried up. The sauce was nice and it goes well with the main meal. Other: I must say they brought the mains out not long after we got our entrees which is okay. Just did n’t really get time to savour the entrees. The serves seems a little all over the place but generally very welcoming and accomodating. Also Incase anyone wants to know they have the MCCS discounts( for marines) Over all my dining experience was fantastic. Just a little disappointed with the takeaway serving sizes. And the wait for takeaway was MUCH longer than the wait for the main meal. Taste for those specific meals( besides the onion and the lack of cinnamon sugar) was great. Will definitely bring my husband and other family here again:)\n",
      "The size of tensor a (574) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.48      0.62       611\n",
      "           1       0.64      0.94      0.76       600\n",
      "\n",
      "    accuracy                           0.71      1211\n",
      "   macro avg       0.77      0.71      0.69      1211\n",
      "weighted avg       0.77      0.71      0.69      1211\n",
      "\n",
      "0.708505367464905\n"
     ]
    }
   ],
   "source": [
    "mapper = {\n",
    "    \"LABEL_0\": 0,\n",
    "    \"LABEL_1\": 1,\n",
    "}\n",
    "utils.apply_kaggle_model(AG6019_comment, mapper, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebf77c",
   "metadata": {},
   "source": [
    "### DT12the/distilbert-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddc9cfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9046109914779663}]\n",
      "[{'label': 'LABEL_0', 'score': 0.9963231086730957}]\n",
      "[{'label': 'LABEL_0', 'score': 0.915234386920929}]\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "DT12the = pipeline(\"text-classification\", model=\"DT12the/distilbert-sentiment-analysis\")\n",
    "print(DT12the(\"I don't like you!\"))\n",
    "print(DT12the(\"this is really good!\"))\n",
    "print(DT12the(\"neutral text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f86d1657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text at index 697: Positives: First time going to this place today. Let me tell you, coming from a family of chefs this was delectable, the dine in meals came out fast, they were LARGE portions, and very good temperature. We ordered the flowered onion( fried and whole), we ordered the Louisiana chook both entrees. Then I had the parmigiana as my main with mash and veg. The mash and veg was perfectly cooked, though the mash tastes a little like packet mash. The sauce with the Louisiana chicken is a little spicy so if you ca n’t tolerate a little spice the sauce is n’t for you. But man oh man the crunch on the chook and the juicy chicken was incredible, was thoroughly enjoyable. The parmigiana was LARGE so much so I could n’t finish it all. Great that they gave takeaways Negatives: The drink I ordered was the summer one in the mocktails section, tasted great only issue I really had was the lemon seeds in the drink, lucky the straws were n’t big enough to suck them up otherwise we would had an issue trying to fish multiple seeds out of the drink. We loved the Louisiana chicken so much we ordered some to take away, sadly to our disappointment the portion sizes were very different, much smaller and not as crunchy still very tasty and very juicy just not the same portion sizes as dine in( specifically regarding our dining experience) We also ordered take away chocolate churro and fruit fondu. The Churros are Exquisite as far as crunch goes but there is little to no cinnamon sugar on it. But the lack of cinnamon sugar can be covered up by the chocolate sauce and fruit eaten together bite by bite. The sauce also tasted Store bought which is still tasted good to me but may not be to everyone liking. The flowered onion, leaves much to be desired for, I definitely would n’t eat it by itself as it essentially just a whole onion fried up. The sauce was nice and it goes well with the main meal. Other: I must say they brought the mains out not long after we got our entrees which is okay. Just did n’t really get time to savour the entrees. The serves seems a little all over the place but generally very welcoming and accomodating. Also Incase anyone wants to know they have the MCCS discounts( for marines) Over all my dining experience was fantastic. Just a little disappointed with the takeaway serving sizes. And the wait for takeaway was MUCH longer than the wait for the main meal. Taste for those specific meals( besides the onion and the lack of cinnamon sugar) was great. Will definitely bring my husband and other family here again:)\n",
      "The size of tensor a (574) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.56      0.70       611\n",
      "           1       0.68      0.97      0.80       600\n",
      "\n",
      "    accuracy                           0.76      1211\n",
      "   macro avg       0.81      0.76      0.75      1211\n",
      "weighted avg       0.82      0.76      0.75      1211\n",
      "\n",
      "0.7605284888521883\n"
     ]
    }
   ],
   "source": [
    "mapper = {\n",
    "    \"LABEL_0\": 1,\n",
    "    \"LABEL_1\": 0,\n",
    "}\n",
    "utils.apply_kaggle_model(DT12the, mapper, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed0d3b",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5c426",
   "metadata": {},
   "source": [
    "### training models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d0956",
   "metadata": {},
   "source": [
    "#### AG6019/reddit-comment-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c990b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "Number of parameters: 66.96M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://huggingface.co/AG6019/reddit-comment-sentiment\n",
    "\n",
    "\n",
    "AG6019_tokenizer = AutoTokenizer.from_pretrained(\"AG6019/reddit-comment-sentiment\")\n",
    "AG6019_model = AutoModelForSequenceClassification.from_pretrained(\"AG6019/reddit-comment-sentiment\", num_labels=2)\n",
    "\n",
    "\n",
    "print(AG6019_model.config.id2label)\n",
    "\n",
    "num_parameters = AG6019_model.num_parameters() / 1_000_000\n",
    "print(f\"Number of parameters: {num_parameters:.2f}M\")\n",
    "\n",
    "\n",
    "train_encodings = utils.tokenize_data(x_train, AG6019_tokenizer)\n",
    "val_encodings = utils.tokenize_data(x_val, AG6019_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce1643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3327' max='3327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3327/3327 58:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.329300</td>\n",
       "      <td>0.286509</td>\n",
       "      <td>0.894389</td>\n",
       "      <td>0.890598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>0.356249</td>\n",
       "      <td>0.890264</td>\n",
       "      <td>0.891251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>0.472964</td>\n",
       "      <td>0.891914</td>\n",
       "      <td>0.891646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainingArguments' object has no attribute 'save_to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m AG6019_tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAG6019_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Save the training arguments\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_json\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAG6019_model/training_args.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainingArguments' object has no attribute 'save_to_json'"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_encodings, y_train)\n",
    "val_dataset = CustomDataset(val_encodings, y_val)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./AG6019_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./AG6019_logs\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"f1\": f1_score(p.label_ids, preds),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=AG6019_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"AG6019_model\")\n",
    "# Save the tokenizer\n",
    "AG6019_tokenizer.save_pretrained(\"AG6019_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7819808",
   "metadata": {},
   "source": [
    "#### siebert/sentiment-roberta-large-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f823654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n",
      "Number of parameters: 355.36M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://huggingface.co/siebert/sentiment-roberta-large-english\n",
    "\n",
    "\n",
    "siebert_tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "siebert_model = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "print(siebert_model.config.id2label)\n",
    "\n",
    "\n",
    "num_parameters = siebert_model.num_parameters() / 1_000_000\n",
    "print(f\"Number of parameters: {num_parameters:.2f}M\")\n",
    "\n",
    "\n",
    "train_encodings = utils.tokenize_data(x_train, siebert_tokenizer)\n",
    "val_encodings = utils.tokenize_data(x_val, siebert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45141532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2218' max='2218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2218/2218 18:32:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.504125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.504125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainingArguments' object has no attribute 'save_to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m siebert_tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msiebert_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Save the training arguments\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_json\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msiebert_model/training_args.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainingArguments' object has no attribute 'save_to_json'"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_encodings, y_train)\n",
    "val_dataset = CustomDataset(val_encodings, y_val)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./siebert_results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./siebert_logs\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"f1\": f1_score(p.label_ids, preds),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=siebert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"siebert_model\")\n",
    "# Save the tokenizer\n",
    "siebert_tokenizer.save_pretrained(\"siebert_model\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6a13a",
   "metadata": {},
   "source": [
    "#### DT12the/distilbert-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7b4d3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "DT12the_tokenizer = AutoTokenizer.from_pretrained(\"DT12the/distilbert-sentiment-analysis\")\n",
    "DT12the_model = AutoModelForSequenceClassification.from_pretrained(\"DT12the/distilbert-sentiment-analysis\", num_labels=2)\n",
    "\n",
    "\n",
    "print(DT12the_model.config.id2label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mapping = {0: 1, 1: 0}\n",
    "y_train_inverted = y_train.map(mapping)\n",
    "y_val_inverted = y_val.map(mapping)\n",
    "\n",
    "\n",
    "\n",
    "num_parameters = DT12the_model.num_parameters() / 1_000_000\n",
    "\n",
    "\n",
    "train_encodings = utils.tokenize_data(x_train, DT12the_tokenizer)\n",
    "val_encodings = utils.tokenize_data(x_val, DT12the_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a08837",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_encodings, y_train_inverted)\n",
    "val_dataset = CustomDataset(val_encodings, y_val_inverted)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./DT12the_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./DT12the_logs\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"f1\": f1_score(p.label_ids, preds),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=DT12the_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"DT12the_model\")\n",
    "# Save the tokenizer\n",
    "DT12the_tokenizer.save_pretrained(\"DT12the_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
