{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b84bcd",
   "metadata": {},
   "source": [
    " Employ [Hugging Face](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=sentiment) transformers for the same classification task as in the first assignment.\n",
    "\n",
    "Explore Hugging Face models to find a pre-trained model that is suitable and promising for fine-tuning to your task. It should make sense to pick one that has been pre-trained for the same language and/or text genre.\n",
    "\n",
    "As a bonus, you can also employ a [domain adaptation](https://huggingface.co/learn/llm-course/chapter7/3?fw=pt) approach, explore [parameter-efficient fine-tuning](https://huggingface.co/docs/peft/main/quicktour) (e.g. LoRA), or [prompting language models](https://huggingface.co/docs/transformers/v4.49.0/en/tasks/prompting).\n",
    "\n",
    "We must ompare the performance of your model(s) with the ones developed for the first assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560a55d",
   "metadata": {},
   "source": [
    "Most of the models have problems processing the text!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ec1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import CustomDataset, CustomDataset1\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493a620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sentiment_df = pd.read_csv(\"../common/data_sentiment_preprocessed.csv\")\n",
    "combined_sentiment_df_val = pd.read_csv(\"../common/data_sentiment_preprocessed_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99de155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = combined_sentiment_df.text\n",
    "y_train = combined_sentiment_df.sentiment_label\n",
    "x_val = combined_sentiment_df_val.text\n",
    "y_val = combined_sentiment_df_val.sentiment_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c4d1f",
   "metadata": {},
   "source": [
    "## Making use of pretrained huggingface models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e32128",
   "metadata": {},
   "source": [
    "### twitter-roberta-base-sentiment-latest\n",
    "This model has positive negative and neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98fef923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9748510718345642}]\n",
      "[{'label': 'negative', 'score': 0.8965417742729187}]\n",
      "[{'label': 'neutral', 'score': 0.7886281609535217}]\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "from transformers import pipeline\n",
    "\n",
    "cardiffnlp_roberta = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "\n",
    "print(cardiffnlp_roberta(\"I love you!\"))\n",
    "print(cardiffnlp_roberta(\"I hate you!\"))\n",
    "print(cardiffnlp_roberta(\"neutral text\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e00cb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text at index 697: Positives: First time going to this place today. Let me tell you, coming from a family of chefs this was delectable, the dine in meals came out fast, they were LARGE portions, and very good temperature. We ordered the flowered onion( fried and whole), we ordered the Louisiana chook both entrees. Then I had the parmigiana as my main with mash and veg. The mash and veg was perfectly cooked, though the mash tastes a little like packet mash. The sauce with the Louisiana chicken is a little spicy so if you ca n’t tolerate a little spice the sauce is n’t for you. But man oh man the crunch on the chook and the juicy chicken was incredible, was thoroughly enjoyable. The parmigiana was LARGE so much so I could n’t finish it all. Great that they gave takeaways Negatives: The drink I ordered was the summer one in the mocktails section, tasted great only issue I really had was the lemon seeds in the drink, lucky the straws were n’t big enough to suck them up otherwise we would had an issue trying to fish multiple seeds out of the drink. We loved the Louisiana chicken so much we ordered some to take away, sadly to our disappointment the portion sizes were very different, much smaller and not as crunchy still very tasty and very juicy just not the same portion sizes as dine in( specifically regarding our dining experience) We also ordered take away chocolate churro and fruit fondu. The Churros are Exquisite as far as crunch goes but there is little to no cinnamon sugar on it. But the lack of cinnamon sugar can be covered up by the chocolate sauce and fruit eaten together bite by bite. The sauce also tasted Store bought which is still tasted good to me but may not be to everyone liking. The flowered onion, leaves much to be desired for, I definitely would n’t eat it by itself as it essentially just a whole onion fried up. The sauce was nice and it goes well with the main meal. Other: I must say they brought the mains out not long after we got our entrees which is okay. Just did n’t really get time to savour the entrees. The serves seems a little all over the place but generally very welcoming and accomodating. Also Incase anyone wants to know they have the MCCS discounts( for marines) Over all my dining experience was fantastic. Just a little disappointed with the takeaway serving sizes. And the wait for takeaway was MUCH longer than the wait for the main meal. Taste for those specific meals( besides the onion and the lack of cinnamon sugar) was great. Will definitely bring my husband and other family here again:)\n",
      "The expanded size of the tensor (574) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 574].  Tensor sizes: [1, 514]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.66      0.78       611\n",
      "           1       0.73      0.97      0.83       600\n",
      "\n",
      "    accuracy                           0.81      1211\n",
      "   macro avg       0.84      0.81      0.81      1211\n",
      "weighted avg       0.84      0.81      0.81      1211\n",
      "\n",
      "0.8100743187448389\n"
     ]
    }
   ],
   "source": [
    "# cardiffnlp_roberta\n",
    "mapper = {\n",
    "    \"negative\": 0,\n",
    "    \"positive\": 1,\n",
    "    \"neutral\": 1\n",
    "}\n",
    "utils.apply_kaggle_model(cardiffnlp_roberta, mapper, x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1c04a",
   "metadata": {},
   "source": [
    "### siebert/sentiment-roberta-large-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf0cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9987329840660095}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9992897510528564}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9969080090522766}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#https://huggingface.co/siebert/sentiment-roberta-large-english?library=transformers\n",
    "\n",
    "\"\"\"\n",
    "    article: https://www.sciencedirect.com/science/article/pii/S0167811622000477\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "siebert_roberta = pipeline(\"text-classification\", model=\"siebert/sentiment-roberta-large-english\")\n",
    "\n",
    "\n",
    "print(siebert_roberta(\"I love you!\"))\n",
    "print(siebert_roberta(\"I hate you!\"))\n",
    "print(siebert_roberta(\"neutral text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fba1a134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text at index 697: Positives: First time going to this place today. Let me tell you, coming from a family of chefs this was delectable, the dine in meals came out fast, they were LARGE portions, and very good temperature. We ordered the flowered onion( fried and whole), we ordered the Louisiana chook both entrees. Then I had the parmigiana as my main with mash and veg. The mash and veg was perfectly cooked, though the mash tastes a little like packet mash. The sauce with the Louisiana chicken is a little spicy so if you ca n’t tolerate a little spice the sauce is n’t for you. But man oh man the crunch on the chook and the juicy chicken was incredible, was thoroughly enjoyable. The parmigiana was LARGE so much so I could n’t finish it all. Great that they gave takeaways Negatives: The drink I ordered was the summer one in the mocktails section, tasted great only issue I really had was the lemon seeds in the drink, lucky the straws were n’t big enough to suck them up otherwise we would had an issue trying to fish multiple seeds out of the drink. We loved the Louisiana chicken so much we ordered some to take away, sadly to our disappointment the portion sizes were very different, much smaller and not as crunchy still very tasty and very juicy just not the same portion sizes as dine in( specifically regarding our dining experience) We also ordered take away chocolate churro and fruit fondu. The Churros are Exquisite as far as crunch goes but there is little to no cinnamon sugar on it. But the lack of cinnamon sugar can be covered up by the chocolate sauce and fruit eaten together bite by bite. The sauce also tasted Store bought which is still tasted good to me but may not be to everyone liking. The flowered onion, leaves much to be desired for, I definitely would n’t eat it by itself as it essentially just a whole onion fried up. The sauce was nice and it goes well with the main meal. Other: I must say they brought the mains out not long after we got our entrees which is okay. Just did n’t really get time to savour the entrees. The serves seems a little all over the place but generally very welcoming and accomodating. Also Incase anyone wants to know they have the MCCS discounts( for marines) Over all my dining experience was fantastic. Just a little disappointed with the takeaway serving sizes. And the wait for takeaway was MUCH longer than the wait for the main meal. Taste for those specific meals( besides the onion and the lack of cinnamon sugar) was great. Will definitely bring my husband and other family here again:)\n",
      "The expanded size of the tensor (574) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 574].  Tensor sizes: [1, 514]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.84      0.88       611\n",
      "           1       0.85      0.93      0.89       600\n",
      "\n",
      "    accuracy                           0.88      1211\n",
      "   macro avg       0.88      0.88      0.88      1211\n",
      "weighted avg       0.88      0.88      0.88      1211\n",
      "\n",
      "0.8810900082576383\n"
     ]
    }
   ],
   "source": [
    "#siebert_roberta\n",
    "mapper = {\n",
    "    \"NEGATIVE\": 0,\n",
    "    \"POSITIVE\": 1\n",
    "} \n",
    "utils.apply_kaggle_model(siebert_roberta, mapper, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd217aff",
   "metadata": {},
   "source": [
    "### saiffff/distilbert-imdb-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dd505e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.7047289609909058}]\n",
      "[{'label': 'LABEL_1', 'score': 0.9926829934120178}]\n",
      "[{'label': 'LABEL_0', 'score': 0.6355293393135071}]\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "saiffff = pipeline(\"text-classification\", model=\"saiffff/distilbert-imdb-sentiment\")\n",
    "print(saiffff(\"I don't like you!\"))\n",
    "print(saiffff(\"this is really good!\"))\n",
    "print(saiffff(\"neutral text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "863a1186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text at index 697: Positives: First time going to this place today. Let me tell you, coming from a family of chefs this was delectable, the dine in meals came out fast, they were LARGE portions, and very good temperature. We ordered the flowered onion( fried and whole), we ordered the Louisiana chook both entrees. Then I had the parmigiana as my main with mash and veg. The mash and veg was perfectly cooked, though the mash tastes a little like packet mash. The sauce with the Louisiana chicken is a little spicy so if you ca n’t tolerate a little spice the sauce is n’t for you. But man oh man the crunch on the chook and the juicy chicken was incredible, was thoroughly enjoyable. The parmigiana was LARGE so much so I could n’t finish it all. Great that they gave takeaways Negatives: The drink I ordered was the summer one in the mocktails section, tasted great only issue I really had was the lemon seeds in the drink, lucky the straws were n’t big enough to suck them up otherwise we would had an issue trying to fish multiple seeds out of the drink. We loved the Louisiana chicken so much we ordered some to take away, sadly to our disappointment the portion sizes were very different, much smaller and not as crunchy still very tasty and very juicy just not the same portion sizes as dine in( specifically regarding our dining experience) We also ordered take away chocolate churro and fruit fondu. The Churros are Exquisite as far as crunch goes but there is little to no cinnamon sugar on it. But the lack of cinnamon sugar can be covered up by the chocolate sauce and fruit eaten together bite by bite. The sauce also tasted Store bought which is still tasted good to me but may not be to everyone liking. The flowered onion, leaves much to be desired for, I definitely would n’t eat it by itself as it essentially just a whole onion fried up. The sauce was nice and it goes well with the main meal. Other: I must say they brought the mains out not long after we got our entrees which is okay. Just did n’t really get time to savour the entrees. The serves seems a little all over the place but generally very welcoming and accomodating. Also Incase anyone wants to know they have the MCCS discounts( for marines) Over all my dining experience was fantastic. Just a little disappointed with the takeaway serving sizes. And the wait for takeaway was MUCH longer than the wait for the main meal. Taste for those specific meals( besides the onion and the lack of cinnamon sugar) was great. Will definitely bring my husband and other family here again:)\n",
      "The size of tensor a (574) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.73      0.79       611\n",
      "           1       0.76      0.87      0.81       600\n",
      "\n",
      "    accuracy                           0.80      1211\n",
      "   macro avg       0.81      0.80      0.80      1211\n",
      "weighted avg       0.81      0.80      0.80      1211\n",
      "\n",
      "0.8018166804293972\n"
     ]
    }
   ],
   "source": [
    "mapper = {\n",
    "    \"LABEL_0\": 0,\n",
    "    \"LABEL_1\": 1,\n",
    "}\n",
    "utils.apply_kaggle_model(saiffff, mapper, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf70bff",
   "metadata": {},
   "source": [
    "### distilbert/distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80756ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "distilbert = pipeline( model=\"distilbert/distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed0d3b",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5c426",
   "metadata": {},
   "source": [
    "### training models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680de3f0",
   "metadata": {},
   "source": [
    "#### saiffff/distilbert-imdb-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5192a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "Number of parameters: 66.96M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://huggingface.co/saiffff/distilbert-imdb-sentiment\n",
    "\n",
    "\n",
    "\n",
    "saiffff_tokenizer = AutoTokenizer.from_pretrained(\"saiffff/distilbert-imdb-sentiment\")\n",
    "saiffff_model = AutoModelForSequenceClassification.from_pretrained(\"saiffff/distilbert-imdb-sentiment\")\n",
    "\n",
    "\n",
    "print(AG6019_model.config.id2label)\n",
    "\n",
    "num_parameters = saiffff_model.num_parameters() / 1_000_000\n",
    "print(f\"Number of parameters: {num_parameters:.2f}M\")\n",
    "\n",
    "\n",
    "train_encodings = utils.tokenize_data(x_train, saiffff_tokenizer)\n",
    "val_encodings = utils.tokenize_data(x_val, saiffff_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95fc90bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3327' max='3327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3327/3327 43:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.331900</td>\n",
       "      <td>0.279159</td>\n",
       "      <td>0.898515</td>\n",
       "      <td>0.895851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.213400</td>\n",
       "      <td>0.391901</td>\n",
       "      <td>0.890264</td>\n",
       "      <td>0.893001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.127300</td>\n",
       "      <td>0.477925</td>\n",
       "      <td>0.891914</td>\n",
       "      <td>0.891286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('saiffff_model/tokenizer_config.json',\n",
       " 'saiffff_model/special_tokens_map.json',\n",
       " 'saiffff_model/vocab.txt',\n",
       " 'saiffff_model/added_tokens.json',\n",
       " 'saiffff_model/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_encodings, y_train)\n",
    "val_dataset = CustomDataset(val_encodings, y_val)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./saiffff_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./saiffff_logs\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"f1\": f1_score(p.label_ids, preds),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=saiffff_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"saiffff_model\")\n",
    "# Save the tokenizer\n",
    "saiffff_tokenizer.save_pretrained(\"saiffff_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43ac4d",
   "metadata": {},
   "source": [
    "### domain adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef1009",
   "metadata": {},
   "source": [
    "#### distilbert/distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cebe09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e936a81b51d84dd0aa5c979c84aaad71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fb3be7dcaa47669827ab60b1c6750b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bfaa4e3e5c457d99fd038f3f3e620b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c2d320ed3b4fd6af0e36ccfcb66a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbff85c63194794b510112d0ac9bd79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d3579162684244abd64d44d4dac38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8866 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60b4ff525bb44908c7f77c986ff49a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "train_dataset = Dataset.from_dict({'text': x_train.tolist(), 'labels': y_train.tolist()})\n",
    "val_dataset = Dataset.from_dict({'text': x_val.tolist(), 'labels': y_val.tolist()})\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column('labels', 'label')\n",
    "tokenized_val_dataset = tokenized_val_dataset.rename_column('labels', 'label')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ece05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1665' max='1665' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1665/1665 09:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>0.269953</td>\n",
       "      <td>0.883663</td>\n",
       "      <td>0.881612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.215800</td>\n",
       "      <td>0.337388</td>\n",
       "      <td>0.878713</td>\n",
       "      <td>0.883055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.148300</td>\n",
       "      <td>0.393756</td>\n",
       "      <td>0.890264</td>\n",
       "      <td>0.889259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete! Model and tokenizer saved to ./distilbert-fine-tuned\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./distilbert_results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    logging_dir='./distilbert_logs',\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='binary'),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model('./distilbert-fine-tuned')\n",
    "tokenizer.save_pretrained('./distilbert-fine-tuned')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9bdf7",
   "metadata": {},
   "source": [
    "### roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df544295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad548a7e5694866aa5084cf60494459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8866 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6dc680ad4b4b2d9a4843d943d5adb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 665,858 || all params: 125,313,028 || trainable%: 0.5314\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "train_dataset = Dataset.from_dict({'text': x_train.tolist(), 'labels': y_train.tolist()})\n",
    "val_dataset = Dataset.from_dict({'text': x_val.tolist(), 'labels': y_val.tolist()})\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column('labels', 'label')\n",
    "tokenized_val_dataset = tokenized_val_dataset.rename_column('labels', 'label')\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62ea1f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5550' max='5550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5550/5550 18:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.516900</td>\n",
       "      <td>0.293446</td>\n",
       "      <td>0.881188</td>\n",
       "      <td>0.878173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.298600</td>\n",
       "      <td>0.300769</td>\n",
       "      <td>0.892739</td>\n",
       "      <td>0.892739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.284221</td>\n",
       "      <td>0.891089</td>\n",
       "      <td>0.888136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.276184</td>\n",
       "      <td>0.895215</td>\n",
       "      <td>0.893367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.261200</td>\n",
       "      <td>0.284259</td>\n",
       "      <td>0.892739</td>\n",
       "      <td>0.893268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.279454</td>\n",
       "      <td>0.899340</td>\n",
       "      <td>0.898502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.257900</td>\n",
       "      <td>0.281307</td>\n",
       "      <td>0.894389</td>\n",
       "      <td>0.894040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.275067</td>\n",
       "      <td>0.895215</td>\n",
       "      <td>0.893724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>0.278791</td>\n",
       "      <td>0.896865</td>\n",
       "      <td>0.896266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>0.278785</td>\n",
       "      <td>0.896040</td>\n",
       "      <td>0.895522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./roberta-fine-tuned/tokenizer_config.json',\n",
       " './roberta-fine-tuned/special_tokens_map.json',\n",
       " './roberta-fine-tuned/vocab.json',\n",
       " './roberta-fine-tuned/merges.txt',\n",
       " './roberta-fine-tuned/added_tokens.json',\n",
       " './roberta-fine-tuned/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./roberta_results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    logging_dir='./roberta_logs',\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='binary'),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model('./roberta-fine-tuned')\n",
    "tokenizer.save_pretrained('./roberta-fine-tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6048d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
