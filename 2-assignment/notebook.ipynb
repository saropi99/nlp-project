{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b84bcd",
   "metadata": {},
   "source": [
    " Employ [Hugging Face](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=sentiment) transformers for the same classification task as in the first assignment.\n",
    "\n",
    "Explore Hugging Face models to find a pre-trained model that is suitable and promising for fine-tuning to your task. It should make sense to pick one that has been pre-trained for the same language and/or text genre.\n",
    "\n",
    "As a bonus, you can also employ a [domain adaptation](https://huggingface.co/learn/llm-course/chapter7/3?fw=pt) approach, explore [parameter-efficient fine-tuning](https://huggingface.co/docs/peft/main/quicktour) (e.g. LoRA), or [prompting language models](https://huggingface.co/docs/transformers/v4.49.0/en/tasks/prompting).\n",
    "\n",
    "We must ompare the performance of your model(s) with the ones developed for the first assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560a55d",
   "metadata": {},
   "source": [
    "Most of the models have problems processing the text!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ec1779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagoromero/Documents/nlp/nlp-env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/santiagoromero/Documents/nlp/nlp-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from utils import CustomDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "493a620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sentiment_df = pd.read_csv(\"../common/data_sentiment_preprocessed.csv\")\n",
    "combined_sentiment_df_val = pd.read_csv(\"../common/data_sentiment_preprocessed_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99de155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = combined_sentiment_df.text\n",
    "y_train = combined_sentiment_df.sentiment_label\n",
    "x_val = combined_sentiment_df_val.text\n",
    "y_val = combined_sentiment_df_val.sentiment_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c4d1f",
   "metadata": {},
   "source": [
    "## Making use of pretrained huggingface models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e32128",
   "metadata": {},
   "source": [
    "### twitter-roberta-base-sentiment-latest\n",
    "This model has positive negative and neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98fef923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9748510718345642}]\n",
      "[{'label': 'negative', 'score': 0.8965417742729187}]\n",
      "[{'label': 'neutral', 'score': 0.7886281609535217}]\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "from transformers import pipeline\n",
    "\n",
    "cardiffnlp_roberta = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "\n",
    "print(cardiffnlp_roberta(\"I love you!\"))\n",
    "print(cardiffnlp_roberta(\"I hate you!\"))\n",
    "print(cardiffnlp_roberta(\"neutral text\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e00cb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text at index 697: Positives: First time going to this place today. Let me tell you, coming from a family of chefs this was delectable, the dine in meals came out fast, they were LARGE portions, and very good temperature. We ordered the flowered onion( fried and whole), we ordered the Louisiana chook both entrees. Then I had the parmigiana as my main with mash and veg. The mash and veg was perfectly cooked, though the mash tastes a little like packet mash. The sauce with the Louisiana chicken is a little spicy so if you ca n’t tolerate a little spice the sauce is n’t for you. But man oh man the crunch on the chook and the juicy chicken was incredible, was thoroughly enjoyable. The parmigiana was LARGE so much so I could n’t finish it all. Great that they gave takeaways Negatives: The drink I ordered was the summer one in the mocktails section, tasted great only issue I really had was the lemon seeds in the drink, lucky the straws were n’t big enough to suck them up otherwise we would had an issue trying to fish multiple seeds out of the drink. We loved the Louisiana chicken so much we ordered some to take away, sadly to our disappointment the portion sizes were very different, much smaller and not as crunchy still very tasty and very juicy just not the same portion sizes as dine in( specifically regarding our dining experience) We also ordered take away chocolate churro and fruit fondu. The Churros are Exquisite as far as crunch goes but there is little to no cinnamon sugar on it. But the lack of cinnamon sugar can be covered up by the chocolate sauce and fruit eaten together bite by bite. The sauce also tasted Store bought which is still tasted good to me but may not be to everyone liking. The flowered onion, leaves much to be desired for, I definitely would n’t eat it by itself as it essentially just a whole onion fried up. The sauce was nice and it goes well with the main meal. Other: I must say they brought the mains out not long after we got our entrees which is okay. Just did n’t really get time to savour the entrees. The serves seems a little all over the place but generally very welcoming and accomodating. Also Incase anyone wants to know they have the MCCS discounts( for marines) Over all my dining experience was fantastic. Just a little disappointed with the takeaway serving sizes. And the wait for takeaway was MUCH longer than the wait for the main meal. Taste for those specific meals( besides the onion and the lack of cinnamon sugar) was great. Will definitely bring my husband and other family here again:)\n",
      "The expanded size of the tensor (574) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 574].  Tensor sizes: [1, 514]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.66      0.78       611\n",
      "           1       0.73      0.97      0.83       600\n",
      "\n",
      "    accuracy                           0.81      1211\n",
      "   macro avg       0.84      0.81      0.81      1211\n",
      "weighted avg       0.84      0.81      0.81      1211\n",
      "\n",
      "0.8100743187448389\n"
     ]
    }
   ],
   "source": [
    "# cardiffnlp_roberta\n",
    "mapper = {\n",
    "    \"negative\": 0,\n",
    "    \"positive\": 1,\n",
    "    \"neutral\": 1\n",
    "}\n",
    "utils.apply_kaggle_model(cardiffnlp_roberta, mapper, x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1c04a",
   "metadata": {},
   "source": [
    "### sentiment-roberta-large-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "badf0cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9987329840660095}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9992897510528564}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9969080090522766}]\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/siebert/sentiment-roberta-large-english?library=transformers\n",
    "\n",
    "\"\"\"\n",
    "    article: https://www.sciencedirect.com/science/article/pii/S0167811622000477\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "siebert_roberta = pipeline(\"text-classification\", model=\"siebert/sentiment-roberta-large-english\")\n",
    "\n",
    "\n",
    "print(siebert_roberta(\"I love you!\"))\n",
    "print(siebert_roberta(\"I hate you!\"))\n",
    "print(siebert_roberta(\"neutral text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba1a134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text at index 697: Positives: First time going to this place today. Let me tell you, coming from a family of chefs this was delectable, the dine in meals came out fast, they were LARGE portions, and very good temperature. We ordered the flowered onion( fried and whole), we ordered the Louisiana chook both entrees. Then I had the parmigiana as my main with mash and veg. The mash and veg was perfectly cooked, though the mash tastes a little like packet mash. The sauce with the Louisiana chicken is a little spicy so if you ca n’t tolerate a little spice the sauce is n’t for you. But man oh man the crunch on the chook and the juicy chicken was incredible, was thoroughly enjoyable. The parmigiana was LARGE so much so I could n’t finish it all. Great that they gave takeaways Negatives: The drink I ordered was the summer one in the mocktails section, tasted great only issue I really had was the lemon seeds in the drink, lucky the straws were n’t big enough to suck them up otherwise we would had an issue trying to fish multiple seeds out of the drink. We loved the Louisiana chicken so much we ordered some to take away, sadly to our disappointment the portion sizes were very different, much smaller and not as crunchy still very tasty and very juicy just not the same portion sizes as dine in( specifically regarding our dining experience) We also ordered take away chocolate churro and fruit fondu. The Churros are Exquisite as far as crunch goes but there is little to no cinnamon sugar on it. But the lack of cinnamon sugar can be covered up by the chocolate sauce and fruit eaten together bite by bite. The sauce also tasted Store bought which is still tasted good to me but may not be to everyone liking. The flowered onion, leaves much to be desired for, I definitely would n’t eat it by itself as it essentially just a whole onion fried up. The sauce was nice and it goes well with the main meal. Other: I must say they brought the mains out not long after we got our entrees which is okay. Just did n’t really get time to savour the entrees. The serves seems a little all over the place but generally very welcoming and accomodating. Also Incase anyone wants to know they have the MCCS discounts( for marines) Over all my dining experience was fantastic. Just a little disappointed with the takeaway serving sizes. And the wait for takeaway was MUCH longer than the wait for the main meal. Taste for those specific meals( besides the onion and the lack of cinnamon sugar) was great. Will definitely bring my husband and other family here again:)\n",
      "The expanded size of the tensor (574) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 574].  Tensor sizes: [1, 514]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.84      0.88       611\n",
      "           1       0.85      0.93      0.89       600\n",
      "\n",
      "    accuracy                           0.88      1211\n",
      "   macro avg       0.88      0.88      0.88      1211\n",
      "weighted avg       0.88      0.88      0.88      1211\n",
      "\n",
      "0.8810900082576383\n"
     ]
    }
   ],
   "source": [
    "#siebert_roberta\n",
    "mapper = {\n",
    "    \"NEGATIVE\": 0,\n",
    "    \"POSITIVE\": 1\n",
    "} \n",
    "utils.apply_kaggle_model(siebert_roberta, mapper, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7099805",
   "metadata": {},
   "source": [
    "### AG6019/reddit-comment-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc8ebe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9998805522918701}]\n",
      "[{'label': 'LABEL_0', 'score': 0.8577982187271118}]\n",
      "[{'label': 'LABEL_0', 'score': 0.9975653886795044}]\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/AG6019/reddit-comment-sentiment?library=transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "AG6019_comment = pipeline(\"text-classification\", model=\"AG6019/reddit-comment-sentiment\")\n",
    "\n",
    "print(AG6019_comment(\"I love you!\"))\n",
    "print(AG6019_comment(\"I dont like you!\"))\n",
    "print(AG6019_comment(\"neutral text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de49a7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text at index 697: Positives: First time going to this place today. Let me tell you, coming from a family of chefs this was delectable, the dine in meals came out fast, they were LARGE portions, and very good temperature. We ordered the flowered onion( fried and whole), we ordered the Louisiana chook both entrees. Then I had the parmigiana as my main with mash and veg. The mash and veg was perfectly cooked, though the mash tastes a little like packet mash. The sauce with the Louisiana chicken is a little spicy so if you ca n’t tolerate a little spice the sauce is n’t for you. But man oh man the crunch on the chook and the juicy chicken was incredible, was thoroughly enjoyable. The parmigiana was LARGE so much so I could n’t finish it all. Great that they gave takeaways Negatives: The drink I ordered was the summer one in the mocktails section, tasted great only issue I really had was the lemon seeds in the drink, lucky the straws were n’t big enough to suck them up otherwise we would had an issue trying to fish multiple seeds out of the drink. We loved the Louisiana chicken so much we ordered some to take away, sadly to our disappointment the portion sizes were very different, much smaller and not as crunchy still very tasty and very juicy just not the same portion sizes as dine in( specifically regarding our dining experience) We also ordered take away chocolate churro and fruit fondu. The Churros are Exquisite as far as crunch goes but there is little to no cinnamon sugar on it. But the lack of cinnamon sugar can be covered up by the chocolate sauce and fruit eaten together bite by bite. The sauce also tasted Store bought which is still tasted good to me but may not be to everyone liking. The flowered onion, leaves much to be desired for, I definitely would n’t eat it by itself as it essentially just a whole onion fried up. The sauce was nice and it goes well with the main meal. Other: I must say they brought the mains out not long after we got our entrees which is okay. Just did n’t really get time to savour the entrees. The serves seems a little all over the place but generally very welcoming and accomodating. Also Incase anyone wants to know they have the MCCS discounts( for marines) Over all my dining experience was fantastic. Just a little disappointed with the takeaway serving sizes. And the wait for takeaway was MUCH longer than the wait for the main meal. Taste for those specific meals( besides the onion and the lack of cinnamon sugar) was great. Will definitely bring my husband and other family here again:)\n",
      "The size of tensor a (574) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.48      0.62       611\n",
      "           1       0.64      0.94      0.76       600\n",
      "\n",
      "    accuracy                           0.71      1211\n",
      "   macro avg       0.77      0.71      0.69      1211\n",
      "weighted avg       0.77      0.71      0.69      1211\n",
      "\n",
      "0.708505367464905\n"
     ]
    }
   ],
   "source": [
    "mapper = {\n",
    "    \"LABEL_0\": 0,\n",
    "    \"LABEL_1\": 1,\n",
    "}\n",
    "utils.apply_kaggle_model(AG6019_comment, mapper, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebf77c",
   "metadata": {},
   "source": [
    "### DT12the/distilbert-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc9cfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9046109914779663}]\n",
      "[{'label': 'LABEL_0', 'score': 0.9963231086730957}]\n",
      "[{'label': 'LABEL_0', 'score': 0.915234386920929}]\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "DT12the = pipeline(\"text-classification\", model=\"DT12the/distilbert-sentiment-analysis\")\n",
    "print(DT12the(\"I don't like you!\"))\n",
    "print(DT12the(\"this is really good!\"))\n",
    "print(DT12the(\"neutral text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f86d1657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text at index 697: Positives: First time going to this place today. Let me tell you, coming from a family of chefs this was delectable, the dine in meals came out fast, they were LARGE portions, and very good temperature. We ordered the flowered onion( fried and whole), we ordered the Louisiana chook both entrees. Then I had the parmigiana as my main with mash and veg. The mash and veg was perfectly cooked, though the mash tastes a little like packet mash. The sauce with the Louisiana chicken is a little spicy so if you ca n’t tolerate a little spice the sauce is n’t for you. But man oh man the crunch on the chook and the juicy chicken was incredible, was thoroughly enjoyable. The parmigiana was LARGE so much so I could n’t finish it all. Great that they gave takeaways Negatives: The drink I ordered was the summer one in the mocktails section, tasted great only issue I really had was the lemon seeds in the drink, lucky the straws were n’t big enough to suck them up otherwise we would had an issue trying to fish multiple seeds out of the drink. We loved the Louisiana chicken so much we ordered some to take away, sadly to our disappointment the portion sizes were very different, much smaller and not as crunchy still very tasty and very juicy just not the same portion sizes as dine in( specifically regarding our dining experience) We also ordered take away chocolate churro and fruit fondu. The Churros are Exquisite as far as crunch goes but there is little to no cinnamon sugar on it. But the lack of cinnamon sugar can be covered up by the chocolate sauce and fruit eaten together bite by bite. The sauce also tasted Store bought which is still tasted good to me but may not be to everyone liking. The flowered onion, leaves much to be desired for, I definitely would n’t eat it by itself as it essentially just a whole onion fried up. The sauce was nice and it goes well with the main meal. Other: I must say they brought the mains out not long after we got our entrees which is okay. Just did n’t really get time to savour the entrees. The serves seems a little all over the place but generally very welcoming and accomodating. Also Incase anyone wants to know they have the MCCS discounts( for marines) Over all my dining experience was fantastic. Just a little disappointed with the takeaway serving sizes. And the wait for takeaway was MUCH longer than the wait for the main meal. Taste for those specific meals( besides the onion and the lack of cinnamon sugar) was great. Will definitely bring my husband and other family here again:)\n",
      "The size of tensor a (574) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.56      0.70       611\n",
      "           1       0.68      0.97      0.80       600\n",
      "\n",
      "    accuracy                           0.76      1211\n",
      "   macro avg       0.81      0.76      0.75      1211\n",
      "weighted avg       0.82      0.76      0.75      1211\n",
      "\n",
      "0.7605284888521883\n"
     ]
    }
   ],
   "source": [
    "mapper = {\n",
    "    \"LABEL_0\": 1,\n",
    "    \"LABEL_1\": 0,\n",
    "}\n",
    "utils.apply_kaggle_model(DT12the, mapper, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed0d3b",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5c426",
   "metadata": {},
   "source": [
    "### training models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7819808",
   "metadata": {},
   "source": [
    "#### siebert/sentiment-roberta-large-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f823654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n",
      "Number of parameters: 355.36M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://huggingface.co/siebert/sentiment-roberta-large-english\n",
    "\n",
    "\n",
    "siebert_tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "siebert_model = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "print(siebert_model.config.id2label)\n",
    "\n",
    "\n",
    "num_parameters = siebert_model.num_parameters() / 1_000_000\n",
    "print(f\"Number of parameters: {num_parameters:.2f}M\")\n",
    "\n",
    "\n",
    "train_encodings = utils.tokenize_data(x_train, siebert_tokenizer)\n",
    "val_encodings = utils.tokenize_data(x_val, siebert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45141532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2218' max='2218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2218/2218 18:32:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.504125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.504125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainingArguments' object has no attribute 'save_to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m siebert_tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msiebert_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Save the training arguments\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_json\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msiebert_model/training_args.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainingArguments' object has no attribute 'save_to_json'"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_encodings, y_train)\n",
    "val_dataset = CustomDataset(val_encodings, y_val)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./siebert_results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./siebert_logs\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"f1\": f1_score(p.label_ids, preds),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=siebert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"siebert_model\")\n",
    "# Save the tokenizer\n",
    "siebert_tokenizer.save_pretrained(\"siebert_model\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d0956",
   "metadata": {},
   "source": [
    "#### AG6019/reddit-comment-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c990b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "Number of parameters: 66.96M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://huggingface.co/AG6019/reddit-comment-sentiment\n",
    "\n",
    "\n",
    "AG6019_tokenizer = AutoTokenizer.from_pretrained(\"AG6019/reddit-comment-sentiment\")\n",
    "AG6019_model = AutoModelForSequenceClassification.from_pretrained(\"AG6019/reddit-comment-sentiment\", num_labels=2)\n",
    "\n",
    "\n",
    "print(AG6019_model.config.id2label)\n",
    "\n",
    "num_parameters = AG6019_model.num_parameters() / 1_000_000\n",
    "print(f\"Number of parameters: {num_parameters:.2f}M\")\n",
    "\n",
    "\n",
    "train_encodings = utils.tokenize_data(x_train, AG6019_tokenizer)\n",
    "val_encodings = utils.tokenize_data(x_val, AG6019_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce1643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3327' max='3327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3327/3327 58:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.329300</td>\n",
       "      <td>0.286509</td>\n",
       "      <td>0.894389</td>\n",
       "      <td>0.890598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>0.356249</td>\n",
       "      <td>0.890264</td>\n",
       "      <td>0.891251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>0.472964</td>\n",
       "      <td>0.891914</td>\n",
       "      <td>0.891646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainingArguments' object has no attribute 'save_to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m AG6019_tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAG6019_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Save the training arguments\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_json\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAG6019_model/training_args.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainingArguments' object has no attribute 'save_to_json'"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_encodings, y_train)\n",
    "val_dataset = CustomDataset(val_encodings, y_val)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./AG6019_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./AG6019_logs\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"f1\": f1_score(p.label_ids, preds),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=AG6019_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"AG6019_model\")\n",
    "# Save the tokenizer\n",
    "AG6019_tokenizer.save_pretrained(\"AG6019_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6a13a",
   "metadata": {},
   "source": [
    "#### DT12the/distilbert-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7b4d3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "DT12the_tokenizer = AutoTokenizer.from_pretrained(\"DT12the/distilbert-sentiment-analysis\")\n",
    "DT12the_model = AutoModelForSequenceClassification.from_pretrained(\"DT12the/distilbert-sentiment-analysis\", num_labels=2)\n",
    "\n",
    "\n",
    "print(DT12the_model.config.id2label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mapping = {0: 1, 1: 0}\n",
    "y_train_inverted = y_train.map(mapping)\n",
    "y_val_inverted = y_val.map(mapping)\n",
    "\n",
    "\n",
    "\n",
    "num_parameters = DT12the_model.num_parameters() / 1_000_000\n",
    "\n",
    "\n",
    "train_encodings = utils.tokenize_data(x_train, DT12the_tokenizer)\n",
    "val_encodings = utils.tokenize_data(x_val, DT12the_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a08837",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy_score(p\u001b[38;5;241m.\u001b[39mlabel_ids, preds),\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1_score(p\u001b[38;5;241m.\u001b[39mlabel_ids, preds),\n\u001b[1;32m     20\u001b[0m     }\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mDT12the_model,\n\u001b[1;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m     33\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDT12the_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/nlp/nlp-env/lib/python3.9/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/nlp/nlp-env/lib/python3.9/site-packages/transformers/trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2558\u001b[0m )\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Documents/nlp/nlp-env/lib/python3.9/site-packages/transformers/trainer.py:3736\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3736\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3738\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3741\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3742\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/nlp/nlp-env/lib/python3.9/site-packages/transformers/trainer.py:3801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3799\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3800\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3801\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/nlp/nlp-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/nlp/nlp-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/nlp/nlp-env/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:1013\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1012\u001b[0m         loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n\u001b[0;32m-> 1013\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1016\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m distilbert_output[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/Documents/nlp/nlp-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/nlp/nlp-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/nlp/nlp-env/lib/python3.9/site-packages/torch/nn/modules/loss.py:821\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/nlp/nlp-env/lib/python3.9/site-packages/torch/nn/functional.py:3639\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3636\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3640\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3641\u001b[0m     )\n\u001b[1;32m   3643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[1;32m   3644\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[1;32m   3645\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 2]))"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_encodings, y_train_inverted)\n",
    "val_dataset = CustomDataset(val_encodings, y_val_inverted)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./DT12the_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./DT12the_logs\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"f1\": f1_score(p.label_ids, preds),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=DT12the_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"DT12the_model\")\n",
    "# Save the tokenizer\n",
    "DT12the_tokenizer.save_pretrained(\"DT12the_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
