{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Sentiment Analysis Classifier\n",
    "\n",
    "##### Group 26: Michal Dawid Kowalski (up202401554) | Pedro Maria Passos Ribeiro do Carmo Pereira (up) | Santiago Romero Pineda (up)\n",
    "\n",
    "In this assignment, we will build a sentiment analysis classifier using traditional machine learning techniques. The process includes pre-processing, feature extraction, and exploring both sparse and dense feature representations like word embeddings. We will use \"traditional\" machine learning classifier instead of deep learning models (CNNs, RNNs, Transformers). The focus will be on understanding text classification techniques and evaluating their performance on the given dataset using common classification metrics like accuracy, precision, recall, and F1-score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import  libraries\n",
    "from our_eda import *\n",
    "from our_modeling import *\n",
    "from our_preprocessing import *\n",
    "from our_feature_extraction import *\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. BESSTIE Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Uploading Dataset Files from HuggingFace (https://huggingface.co/mindhunter23)\n",
    "\n",
    "The dataset is hosted on Hugging Face under the username \"mindhunter23.\" It consists of text data collected from Reddit and Google for the countries UK, AU, and IN. All texts are in English and are labeled with sentiment values: 0 for negative sentiment and 1 for positive sentiment. The dataset is already split into training and validation sets, making it ready for sentiment analysis tasks. It offers diverse content from different regions and platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - BESSTIE-reddit-sentiment-uk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1cimjpr</td>\n",
       "      <td>So instead of making savings, they continued t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1d35qlg</td>\n",
       "      <td>Needless story to have dragged into the electi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1d3i3mt</td>\n",
       "      <td>Now, in an ideal world there would be insight ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1d5a8wa</td>\n",
       "      <td>How did you not get mind controlled at birth t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1d5l3e9</td>\n",
       "      <td>Talk lately of conscription, having a store of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>1b5iodf</td>\n",
       "      <td>How is this a non-story? A shop will bow to th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>1cv6iym</td>\n",
       "      <td>The smoke screen is real, and Suella (along wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1c9kjse</td>\n",
       "      <td>It's a really serious problem that young white...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>1b22zae</td>\n",
       "      <td>Good luck. Imagine if you refuse to see a PA b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>1d3zv41</td>\n",
       "      <td>So first thing you're going to do is to fix th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1007 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  \\\n",
       "0     1cimjpr  So instead of making savings, they continued t...   \n",
       "1     1d35qlg  Needless story to have dragged into the electi...   \n",
       "2     1d3i3mt  Now, in an ideal world there would be insight ...   \n",
       "3     1d5a8wa  How did you not get mind controlled at birth t...   \n",
       "4     1d5l3e9  Talk lately of conscription, having a store of...   \n",
       "...       ...                                                ...   \n",
       "1002  1b5iodf  How is this a non-story? A shop will bow to th...   \n",
       "1003  1cv6iym  The smoke screen is real, and Suella (along wi...   \n",
       "1004  1c9kjse  It's a really serious problem that young white...   \n",
       "1005  1b22zae  Good luck. Imagine if you refuse to see a PA b...   \n",
       "1006  1d3zv41  So first thing you're going to do is to fix th...   \n",
       "\n",
       "      sentiment_label  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "1002                0  \n",
       "1003                0  \n",
       "1004                0  \n",
       "1005                0  \n",
       "1006                0  \n",
       "\n",
       "[1007 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'reddit-sentiment-uk-train.jsonl', 'validation': 'reddit-sentiment-uk-valid.jsonl'}\n",
    "df_reddit_sentiment_uk = pd.read_json(\"hf://datasets/mindhunter23/BESSTIE-reddit-sentiment-uk/\" + splits[\"train\"], lines=True)\n",
    "df_reddit_sentiment_uk_val = pd.read_json(\"hf://datasets/mindhunter23/BESSTIE-reddit-sentiment-uk/\" + splits[\"validation\"], lines=True)\n",
    "df_reddit_sentiment_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count  Percentage\n",
      "0    892       88.58\n",
      "1    115       11.42\n"
     ]
    }
   ],
   "source": [
    "class_distribution(df_reddit_sentiment_uk) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - BESSTIE-reddit-sentiment-au/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1d2d56d</td>\n",
       "      <td>No its more about risk management, why accept ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1d2cfsd</td>\n",
       "      <td>I don’t play this game. \\n\\nThem: “What are yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1cw9vcr</td>\n",
       "      <td>Well I'm not really confident that we'll see m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1czvemb</td>\n",
       "      <td>He's not wrong though  alot of media is RW sla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1d3x6bo</td>\n",
       "      <td>Please contact safe transport Victoria. This i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>1d3yzbw</td>\n",
       "      <td>I just went through this with my tenant who wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>1d3yn9l</td>\n",
       "      <td>Yeah my rental has a shitty little 1kw split s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>1d0upb1</td>\n",
       "      <td>Lol like what ? go back in time and buy 150mil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>1d639g5</td>\n",
       "      <td>From my point of view, there is absolutely not...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>1d00sm5</td>\n",
       "      <td>[Billy Snedden]( The speaker of the house/lead...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1763 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  \\\n",
       "0     1d2d56d  No its more about risk management, why accept ...   \n",
       "1     1d2cfsd  I don’t play this game. \\n\\nThem: “What are yo...   \n",
       "2     1cw9vcr  Well I'm not really confident that we'll see m...   \n",
       "3     1czvemb  He's not wrong though  alot of media is RW sla...   \n",
       "4     1d3x6bo  Please contact safe transport Victoria. This i...   \n",
       "...       ...                                                ...   \n",
       "1758  1d3yzbw  I just went through this with my tenant who wa...   \n",
       "1759  1d3yn9l  Yeah my rental has a shitty little 1kw split s...   \n",
       "1760  1d0upb1  Lol like what ? go back in time and buy 150mil...   \n",
       "1761  1d639g5  From my point of view, there is absolutely not...   \n",
       "1762  1d00sm5  [Billy Snedden]( The speaker of the house/lead...   \n",
       "\n",
       "      sentiment_label  \n",
       "0                   1  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   1  \n",
       "...               ...  \n",
       "1758                1  \n",
       "1759                0  \n",
       "1760                0  \n",
       "1761                0  \n",
       "1762                0  \n",
       "\n",
       "[1763 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {'train': 'reddit-sentiment-au-train.jsonl', 'validation': 'reddit-sentiment-au-valid.jsonl'}\n",
    "df_reddit_sentiment_au = pd.read_json(\"hf://datasets/mindhunter23/BESSTIE-reddit-sentiment-au/\" + splits[\"train\"], lines=True)\n",
    "df_reddit_sentiment_au_val = pd.read_json(\"hf://datasets/mindhunter23/BESSTIE-reddit-sentiment-au/\" + splits[\"validation\"], lines=True)\n",
    "df_reddit_sentiment_au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count  Percentage\n",
      "0   1200       68.07\n",
      "1    563       31.93\n"
     ]
    }
   ],
   "source": [
    "class_distribution(df_reddit_sentiment_au)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - BESSTIE-google-sentiment-uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.046000e+20</td>\n",
       "      <td>Tricky me because I was checking in over midni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.161344e+20</td>\n",
       "      <td>It's lots more cheaper than the Odeon although...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.034757e+20</td>\n",
       "      <td>My first time and last time in this place. It ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.073389e+20</td>\n",
       "      <td>You know, its not bad at all, you get plenty o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.172204e+20</td>\n",
       "      <td>It's. It's OK for a quick fix of junk food. Re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>1.156926e+20</td>\n",
       "      <td>Great service by the staff! The food was good ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>1.087021e+20</td>\n",
       "      <td>Food was delicious chicken bacon avocado in br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>1.176660e+20</td>\n",
       "      <td>It was a very nice moon display with some love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>1.114779e+20</td>\n",
       "      <td>The food was nice but the service from one sta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>1.128380e+20</td>\n",
       "      <td>Wow, what a disappointment. Came as a party of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1817 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                               text  \\\n",
       "0     1.046000e+20  Tricky me because I was checking in over midni...   \n",
       "1     1.161344e+20  It's lots more cheaper than the Odeon although...   \n",
       "2     1.034757e+20  My first time and last time in this place. It ...   \n",
       "3     1.073389e+20  You know, its not bad at all, you get plenty o...   \n",
       "4     1.172204e+20  It's. It's OK for a quick fix of junk food. Re...   \n",
       "...            ...                                                ...   \n",
       "1812  1.156926e+20  Great service by the staff! The food was good ...   \n",
       "1813  1.087021e+20  Food was delicious chicken bacon avocado in br...   \n",
       "1814  1.176660e+20  It was a very nice moon display with some love...   \n",
       "1815  1.114779e+20  The food was nice but the service from one sta...   \n",
       "1816  1.128380e+20  Wow, what a disappointment. Came as a party of...   \n",
       "\n",
       "      sentiment_label  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   0  \n",
       "3                   1  \n",
       "4                   0  \n",
       "...               ...  \n",
       "1812                1  \n",
       "1813                1  \n",
       "1814                1  \n",
       "1815                1  \n",
       "1816                0  \n",
       "\n",
       "[1817 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {'train': 'google-sentiment-uk-train.jsonl', 'validation': 'google-sentiment-uk-valid.jsonl'}\n",
    "df_google_sentiment_uk = pd.read_json(\"hf://datasets/mindhunter23/BESSTIE-google-sentiment-uk/\" + splits[\"train\"], lines=True)\n",
    "df_google_sentiment_uk_val = pd.read_json(\"hf://datasets/mindhunter23/BESSTIE-google-sentiment-uk/\" + splits[\"validation\"], lines=True)\n",
    "df_google_sentiment_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count  Percentage\n",
      "1   1359       74.79\n",
      "0    458       25.21\n"
     ]
    }
   ],
   "source": [
    "class_distribution(df_google_sentiment_uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - BESSTIE-google-sentiment-au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.132555e+20</td>\n",
       "      <td>This was one of the best dishes I've EVER had!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.101411e+20</td>\n",
       "      <td>This Mexican restaurant in Penrith is a great ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.103038e+20</td>\n",
       "      <td>This was not to bad, I ordered the big pork ri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.107520e+20</td>\n",
       "      <td>Clean cool and a nice smaller casino to check ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.152390e+20</td>\n",
       "      <td>Well set out. Great areas to enjoy. Good food ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>1.087996e+20</td>\n",
       "      <td>Beautiful meals and fast cocktails. Waitress w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>1.044438e+20</td>\n",
       "      <td>With a reputation for great food and terrific ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>1.134915e+20</td>\n",
       "      <td>Nice movie theatre, only downfall are the seat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>1.073785e+20</td>\n",
       "      <td>A beautifully styled space with the fun of a s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>1.050974e+20</td>\n",
       "      <td>Went for Xmas lunch with friends... previous c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>946 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                               text  \\\n",
       "0    1.132555e+20  This was one of the best dishes I've EVER had!...   \n",
       "1    1.101411e+20  This Mexican restaurant in Penrith is a great ...   \n",
       "2    1.103038e+20  This was not to bad, I ordered the big pork ri...   \n",
       "3    1.107520e+20  Clean cool and a nice smaller casino to check ...   \n",
       "4    1.152390e+20  Well set out. Great areas to enjoy. Good food ...   \n",
       "..            ...                                                ...   \n",
       "941  1.087996e+20  Beautiful meals and fast cocktails. Waitress w...   \n",
       "942  1.044438e+20  With a reputation for great food and terrific ...   \n",
       "943  1.134915e+20  Nice movie theatre, only downfall are the seat...   \n",
       "944  1.073785e+20  A beautifully styled space with the fun of a s...   \n",
       "945  1.050974e+20  Went for Xmas lunch with friends... previous c...   \n",
       "\n",
       "     sentiment_label  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  \n",
       "..               ...  \n",
       "941                1  \n",
       "942                1  \n",
       "943                1  \n",
       "944                1  \n",
       "945                0  \n",
       "\n",
       "[946 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {'train': 'data/google-sentiment-au-train.jsonl', 'validation': 'data/google-sentiment-au-valid.jsonl'}\n",
    "df_google_sentiment_au = pd.read_json(\"hf://datasets/mindhunter23/BESSTIE-google-sentiment-au/\" + splits[\"train\"], lines=True)\n",
    "df_google_sentiment_au_val = pd.read_json(\"hf://datasets/mindhunter23/BESSTIE-google-sentiment-au/\" + splits[\"validation\"], lines=True)\n",
    "df_google_sentiment_au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count  Percentage\n",
      "1    695       73.47\n",
      "0    251       26.53\n"
     ]
    }
   ],
   "source": [
    "class_distribution(df_google_sentiment_au)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - BESSTIE-reddit-sentiment-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1d2o00l</td>\n",
       "      <td>Zepto has a mandate that the delivery boy need...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1d5fcvf</td>\n",
       "      <td>Mujhe bhi thoda paisa do</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1d04uk7</td>\n",
       "      <td>Nooo don't protest against secular freedom fig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1d5dl6q</td>\n",
       "      <td>Har 3 mahine baad kisi bhi global celebrity ko...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1d66tng</td>\n",
       "      <td>Just because you don't find anything serious b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>1d4p9vp</td>\n",
       "      <td>Rectal Cargo.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>1d3wg8h</td>\n",
       "      <td>I mean this is the equivalent of u go to US an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>1czj077</td>\n",
       "      <td>what went wrong with the genes after Rajiv Gan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>1d43657</td>\n",
       "      <td>The image says the data from 2016-2017. 8 year...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>1cxzgh6</td>\n",
       "      <td>What is this joke?\\n\\nThis guy killed two peop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1685 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  \\\n",
       "0     1d2o00l  Zepto has a mandate that the delivery boy need...   \n",
       "1     1d5fcvf                           Mujhe bhi thoda paisa do   \n",
       "2     1d04uk7  Nooo don't protest against secular freedom fig...   \n",
       "3     1d5dl6q  Har 3 mahine baad kisi bhi global celebrity ko...   \n",
       "4     1d66tng  Just because you don't find anything serious b...   \n",
       "...       ...                                                ...   \n",
       "1680  1d4p9vp                                      Rectal Cargo.   \n",
       "1681  1d3wg8h  I mean this is the equivalent of u go to US an...   \n",
       "1682  1czj077  what went wrong with the genes after Rajiv Gan...   \n",
       "1683  1d43657  The image says the data from 2016-2017. 8 year...   \n",
       "1684  1cxzgh6  What is this joke?\\n\\nThis guy killed two peop...   \n",
       "\n",
       "      sentiment_label  \n",
       "0                   1  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "1680                0  \n",
       "1681                0  \n",
       "1682                0  \n",
       "1683                0  \n",
       "1684                0  \n",
       "\n",
       "[1685 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {'train': 'reddit-sentiment-in-train.jsonl', 'validation': 'reddit-sentiment-in-valid.jsonl'}\n",
    "df_reddit_sentiment_in = pd.read_json(\"hf://datasets/mindhunter23/BESSTIE-reddit-sentiment-in/\" + splits[\"train\"], lines=True)\n",
    "df_reddit_sentiment_in_val = pd.read_json(\"hf://datasets/mindhunter23/BESSTIE-reddit-sentiment-in/\" + splits[\"validation\"], lines=True)\n",
    "df_reddit_sentiment_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count  Percentage\n",
      "0   1256       74.54\n",
      "1    429       25.46\n"
     ]
    }
   ],
   "source": [
    "class_distribution(df_reddit_sentiment_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - BESSTIE-google-sentiment-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.114268e+20</td>\n",
       "      <td>They have an amazing hospitality structure loc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.116605e+20</td>\n",
       "      <td>The attender attitude is not welcoming. Ordere...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.134214e+20</td>\n",
       "      <td>The taste is good.. Decent staff.. But the atm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.055034e+20</td>\n",
       "      <td>Wahi purani jagah, wahi purani yaadein..... Ra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.092858e+20</td>\n",
       "      <td>An extremely over hyped biriyani, Definitely i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>1.045586e+20</td>\n",
       "      <td>Location wise it's Good. Parking at road side ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>1.051527e+20</td>\n",
       "      <td>The experience was good, the atmosphere also g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>1.032750e+20</td>\n",
       "      <td>I tried the laxmi special sandwich double chee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>1.152006e+20</td>\n",
       "      <td>I love o yes but recently they were done blund...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>1.071810e+20</td>\n",
       "      <td>Lovely place for spending time with your loved...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1648 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                               text  \\\n",
       "0     1.114268e+20  They have an amazing hospitality structure loc...   \n",
       "1     1.116605e+20  The attender attitude is not welcoming. Ordere...   \n",
       "2     1.134214e+20  The taste is good.. Decent staff.. But the atm...   \n",
       "3     1.055034e+20  Wahi purani jagah, wahi purani yaadein..... Ra...   \n",
       "4     1.092858e+20  An extremely over hyped biriyani, Definitely i...   \n",
       "...            ...                                                ...   \n",
       "1643  1.045586e+20  Location wise it's Good. Parking at road side ...   \n",
       "1644  1.051527e+20  The experience was good, the atmosphere also g...   \n",
       "1645  1.032750e+20  I tried the laxmi special sandwich double chee...   \n",
       "1646  1.152006e+20  I love o yes but recently they were done blund...   \n",
       "1647  1.071810e+20  Lovely place for spending time with your loved...   \n",
       "\n",
       "      sentiment_label  \n",
       "0                   1  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   1  \n",
       "4                   1  \n",
       "...               ...  \n",
       "1643                0  \n",
       "1644                1  \n",
       "1645                1  \n",
       "1646                0  \n",
       "1647                1  \n",
       "\n",
       "[1648 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {'train': 'google-sentiment-in-train.jsonl', 'validation': 'google-sentiment-in-valid.jsonl'}\n",
    "df_google_sentiment_in = pd.read_json(\"hf://datasets/mindhunter23/BESSTIE-google-sentiment-in/\" + splits[\"train\"], lines=True)\n",
    "df_google_sentiment_in_val = pd.read_json(\"hf://datasets/mindhunter23/BESSTIE-google-sentiment-in/\" + splits[\"validation\"], lines=True)\n",
    "df_google_sentiment_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count  Percentage\n",
      "1   1232       74.76\n",
      "0    416       25.24\n"
     ]
    }
   ],
   "source": [
    "class_distribution(df_google_sentiment_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initial Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Testing text_preprocess() func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " So instead of making savings, they continued to spend money they didn’t have, yes that sounds very responsible. Maybe if the government had continued spending, the whole country would be in the same financial mess Birmingham is in. \n",
      "\n",
      "Lemma. with stopwords:\n",
      " so instead of make saving they continue to spend money they do not have yes that sound very responsible maybe if the government have continue spending the whole country would be in the same financial mess birmingham be in \n",
      "\n",
      "Lemma. without stopwords:\n",
      " instead make saving continue spend money not yes sound responsible maybe government continue spending whole country would financial mess birmingham \n",
      "\n",
      "Stemmer with stopwords:\n",
      " so instead of make save they continu to spend money they did not have ye that sound veri respons mayb if the govern had continu spend the whole countri would be in the same financi mess birmingham is in \n",
      "\n",
      "Stemmer without stopwords:\n",
      " instead make save continu spend money not ye sound veri respons mayb govern continu spend whole countri would financi mess birmingham \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the preprocessing function \n",
    "print('Original:\\n', df_reddit_sentiment_uk.loc[0].text,'\\n')\n",
    "print('Lemmatization:\\n',text_preprocess(df_reddit_sentiment_uk.loc[0].text, remove_digits=True, stemmer=Stemmer.WordNet),'\\n')\n",
    "print('Stemming with stopwords:\\n',text_preprocess(df_reddit_sentiment_uk.loc[0].text),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Concatening datasets\n",
    "### SENTIMENT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in combined dataset: 8866\n",
      "\n",
      "Class distribution:\n",
      "\n",
      "   Count  Percentage\n",
      "0   4473       50.45\n",
      "1   4393       49.55\n",
      "\n",
      "Dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1cimjpr</td>\n",
       "      <td>So instead of making savings, they continued t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1d35qlg</td>\n",
       "      <td>Needless story to have dragged into the electi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1d3i3mt</td>\n",
       "      <td>Now, in an ideal world there would be insight ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1d5a8wa</td>\n",
       "      <td>How did you not get mind controlled at birth t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1d5l3e9</td>\n",
       "      <td>Talk lately of conscription, having a store of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  sentiment_label\n",
       "0  1cimjpr  So instead of making savings, they continued t...                0\n",
       "1  1d35qlg  Needless story to have dragged into the electi...                0\n",
       "2  1d3i3mt  Now, in an ideal world there would be insight ...                0\n",
       "3  1d5a8wa  How did you not get mind controlled at birth t...                0\n",
       "4  1d5l3e9  Talk lately of conscription, having a store of...                0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assue all datasets are already loaded as DataFrames\n",
    "combined_sentiment_df = pd.concat(\n",
    "    [\n",
    "        df_reddit_sentiment_uk,\n",
    "        df_reddit_sentiment_au,\n",
    "        df_google_sentiment_uk,\n",
    "        df_google_sentiment_au,\n",
    "        df_reddit_sentiment_in,\n",
    "        df_google_sentiment_in\n",
    "    ],\n",
    "    axis=0,  # Concatenate vertically (row-wise)\n",
    "    ignore_index=True  # Reset the index in the combined DataFrame\n",
    ")\n",
    "\n",
    "# Assue all datasets are already loaded as DataFrames\n",
    "combined_sentiment_df_val = pd.concat(\n",
    "    [\n",
    "        df_reddit_sentiment_uk_val,\n",
    "        df_reddit_sentiment_au_val,\n",
    "        df_google_sentiment_uk_val,\n",
    "        df_google_sentiment_au_val,\n",
    "        df_reddit_sentiment_in_val,\n",
    "        df_google_sentiment_in_val\n",
    "    ],\n",
    "    axis=0,  # Concatenate vertically (row-wise)\n",
    "    ignore_index=True  # Reset the index in the combined DataFrame\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sentiment_df = pd.read_csv(\"data_sentiment_preprocessed.csv\")\n",
    "combined_sentiment_df_val = pd.read_csv(\"data_sentiment_preprocessed_val.csv\")\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Training Dataset\\n\")\n",
    "print(combined_sentiment_df.head())\n",
    "print(f\"Total rows in combined dataset: {len(combined_sentiment_df)}\")\n",
    "class_distribution(combined_sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the combined DataFrame\n",
    "print(\"Validation Dataset\\n\")\n",
    "print(combined_sentiment_df_val.head())\n",
    "print(f\"Total rows in combined dataset: {len(combined_sentiment_df_val)}\")\n",
    "class_distribution(combined_sentiment_df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of characters per review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Char Count')\n",
    "plt.ylabel('Sample Count')\n",
    "combined_sentiment_df['text'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\n",
    "ax1.hist(combined_sentiment_df[combined_sentiment_df['sentiment_label']==1]['text'].str.len())\n",
    "ax1.set_title( 'Positive Reviews')\n",
    "ax1.set_xlabel('Char Count')\n",
    "ax2.hist(combined_sentiment_df[combined_sentiment_df['sentiment_label']==0]['text'].str.len())\n",
    "ax2.set_title( 'Negative Reviews')\n",
    "ax2.set_xlabel('Char Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSITIVE SENTIMENT\n",
    "text = \" \".join(i for i in combined_sentiment_df[combined_sentiment_df['sentiment_label']==1]['text'])\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Wordcloud for positive review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEGATIVE SENTIMENT\n",
    "text = \" \".join(i for i in combined_sentiment_df[combined_sentiment_df['sentiment_label']==0]['text'])\n",
    "wordcloud = WordCloud( background_color=\"white\").generate(text)\n",
    "\n",
    "plt.figure( figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Wordcloud for negative review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing + Lemmatization \n",
    "combined_sentiment_df['clean_text'] = combined_sentiment_df['text'].apply(lambda x: text_preprocess(x, remove_digits=True, stemmer=Stemmer.WordNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "combined_sentiment_df['tokenized_text'] = combined_sentiment_df['clean_text'].apply(lambda x: word_tokenize(x))\n",
    "combined_sentiment_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed training data\n",
    "combined_sentiment_df.to_csv('data_sentiment_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing + Lemmatization \n",
    "combined_sentiment_df_val['clean_text'] = combined_sentiment_df_val['text'].apply(lambda x: text_preprocess(x, remove_digits=True, stemmer=Stemmer.WordNet))\n",
    "# Tokenization\n",
    "combined_sentiment_df_val['tokenized_text'] = combined_sentiment_df_val['clean_text'].apply(lambda x: word_tokenize(x))\n",
    "combined_sentiment_df_val.head(5)\n",
    "# Save preprocessed validation data\n",
    "combined_sentiment_df_val.to_csv('data_sentiment_preprocessed_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id     text   sentiment_label  clean_text\n",
      "False  False  False            False         8860\n",
      "                               True             6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(combined_sentiment_df.isnull().value_counts())\n",
    "combined_sentiment_df = combined_sentiment_df.dropna() # Drop rows where preprocessing didnt extract any tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id     text   sentiment_label  clean_text\n",
      "False  False  False            False         1211\n",
      "                               True             1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(combined_sentiment_df_val.isnull().value_counts())\n",
    "combined_sentiment_df_val = combined_sentiment_df_val.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read preprocessed datasets from .csv files\n",
    "combined_sentiment_df = pd.read_csv(\"data_sentiment_preprocessed.csv\")\n",
    "combined_sentiment_df_val = pd.read_csv(\"data_sentiment_preprocessed_val.csv\")\n",
    "\n",
    "from our_feature_extraction import basic_bag, tf_idf\n",
    "# Split the data\n",
    "X_train = combined_sentiment_df.tokenized_text\n",
    "y_train = combined_sentiment_df.sentiment_label\n",
    "X_val = combined_sentiment_df_val.tokenized_text\n",
    "y_val = combined_sentiment_df_val.sentiment_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Basic BoW\n",
    "+ removing words that occurs less than 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (X_train_vec) before reuction:  (8860, 13942)\n",
      "Shape (X_train_vec) after reuction:  (8860, 6231)\n",
      "Shape (X_val_vec):  (1211, 6231)\n"
     ]
    }
   ],
   "source": [
    "word_counts, vocab, selected_words, vectorizer, X_train_vec, X_val_vec = basic_bag(X_train, X_val, min_refs=3, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words:\n",
      "not: 4523\n",
      "good: 3415\n",
      "food: 2608\n",
      "get: 1843\n",
      "go: 1610\n",
      "place: 1569\n",
      "like: 1549\n",
      "would: 1527\n",
      "time: 1374\n",
      "one: 1363\n"
     ]
    }
   ],
   "source": [
    "# 10 most common words\n",
    "word_counts = np.asarray(X_train_vec.sum(axis=0)).flatten()\n",
    "vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "top_indices = np.argsort(word_counts)[::-1]\n",
    "top_words = vocab[top_indices[:10]]\n",
    "top_counts = word_counts[top_indices[:10]]\n",
    "\n",
    "print('Most common words\\n:')\n",
    "for word, count in zip(top_words, top_counts):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just test\n",
    "np.unique(X_train_vec[2].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 1-hot BoW\n",
    "+ removing words that occurs less than 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (X_train_hot):  (8860, 6231)\n",
      "Shape (X_val_hot):  (1211, 6231)\n"
     ]
    }
   ],
   "source": [
    "word_counts, vocab, selected_words, vectorizer, X_train_hot, X_val_hot = basic_bag(X_train, X_val, min_refs=3, ohe=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Checking if dataset is binary\n",
    "unique = np.unique(X_train_hot.toarray())\n",
    "print('Unique values:', unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (X_train_tf) after reuction:  (8860, 6231)\n",
      "Shape (X_val_tf):  (1211, 6231)\n"
     ]
    }
   ],
   "source": [
    "word_counts, vocab, selected_words, vectorizer, X_train_vec_tf, X_val_vec_tf = tf_idf(X_train, X_val, min_refs=3, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 N-grams\n",
    "\n",
    "### 5.4.1 Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (X_train_bi):  (8860, 21711)\n",
      "Shape (X_val_bi):  (1211, 21711)\n"
     ]
    }
   ],
   "source": [
    "word_counts, vocab, selected_words, vectorizer, X_train_vec_bi, X_val_vec_bi = basic_bag(X_train, X_val, ngram_range=(2,2), min_refs=3, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common bigrams:\n",
      "food good: 194\n",
      "staff friendly: 160\n",
      "not good: 145\n",
      "really good: 128\n",
      "good place: 126\n",
      "good food: 124\n",
      "look like: 122\n",
      "taste good: 118\n",
      "not even: 110\n",
      "service good: 109\n"
     ]
    }
   ],
   "source": [
    "bigram_vocab = vectorizer.get_feature_names_out()\n",
    "bigram_counts = np.asarray(X_train_vec_bi.sum(axis=0)).flatten()\n",
    "\n",
    "bigram_freq = list(zip(bigram_vocab, bigram_counts))\n",
    "\n",
    "# Soritng\n",
    "sorted_bigram_freq = sorted(bigram_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"10 most common bigrams\\n:\")\n",
    "for bigram, count in sorted_bigram_freq[:10]:\n",
    "    print(f\"{bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Words Embedding (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Basic BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb(X_train_vec, X_val_vec, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 1-hot BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb(X_train_hot, X_val_hot, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb(X_train_vec_tf, X_val_vec_tf, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb(X_train_vec_bi, X_val_vec_bi, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Basic BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_vec, X_val_vec, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 1-hot BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_hot, X_val_hot, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_vec_tf, X_val_vec_tf, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_vec_bi, X_val_vec_bi, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Basic BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(X_train_vec, X_val_vec, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 1-hot BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(X_train_hot, X_val_hot, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(X_train_vec_tf, X_val_vec_tf, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(X_train_vec_bi, X_val_vec_bi, y_train, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
